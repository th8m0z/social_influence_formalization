# Find a more reasonable range for x-axis
# Calculate the 1st and 99th percentiles across all estimates to avoid extreme outliers
all_values <- unlist(all_estimates)
q_low <- quantile(all_values, 0.01)
q_high <- quantile(all_values, 0.99)
# Set range to include true value and reasonable range of estimates
x_min <- min(q_low, true_value * 0.5)
x_max <- max(q_high, true_value * 1.5)
# Create an empty plot
plot(1, type="n", xlim=c(x_min, x_max), ylim=c(0, 0.03),
xlab="Estimate Value", ylab="Probability Density",
main="Distribution of Estimates by Prior Knowledge Level")
# Add explanation text about density
mtext("Higher peaks = more concentrated estimates", side=3, line=0.5, cex=0.8)
# Add the density curves for each knowledge level
legend_text <- character(length(knowledge_levels))
for (i in 1:length(knowledge_levels)) {
pk <- knowledge_levels[i]
d <- density(all_estimates[[i]], adjust=1.2)  # Slightly smoother curves
lines(d, col=colors[i], lwd=2)
# Calculate mean and standard deviation for this knowledge level
mean_val <- mean(all_estimates[[i]])
sd_val <- sd(all_estimates[[i]])
legend_text[i] <- sprintf("PK = %.1f (Mean = %.1f, SD = %.1f)",
pk, mean_val, sd_val)
}
# Add a vertical line for the true value
abline(v=true_value, lty=2, col="black", lwd=2)
text(true_value, 0, "True Value", pos=3, offset=0.5)
# Add a legend with more information
legend("topright", legend=legend_text, col=colors, lwd=2, cex=0.9)
# Return the data for potential further analysis
invisible(all_estimates)
}
plot_estimate_distributions()
# Function to plot distribution of estimates by knowledge level
plot_estimate_distributions <- function(true_value = 100, n_samples = 1000,
knowledge_levels = c(0.1, 0.5, 0.9)) {
# Create an empty list to store the estimates for each knowledge level
all_estimates <- list()
# Generate estimates for each knowledge level
for (pk in knowledge_levels) {
# Generate n_samples estimates for this knowledge level
estimates <- sapply(rep(pk, n_samples), dIndividualFirstEstimate,
true_value = true_value)
# Store the estimates
all_estimates[[as.character(pk)]] <- estimates
}
# Set up the plotting area with multiple colors
colors <- c("red", "green", "blue")[1:length(knowledge_levels)]
# Find a more reasonable range for x-axis
# Calculate the 1st and 99th percentiles across all estimates to avoid extreme outliers
all_values <- unlist(all_estimates)
q_low <- quantile(all_values, 0.01)
q_high <- quantile(all_values, 0.99)
# Set range to include true value and reasonable range of estimates
x_min <- min(q_low, true_value * 0.5)
x_max <- max(q_high, true_value * 1.5)
# Create an empty plot
plot(1, type="n", xlim=c(x_min, x_max), ylim=c(0, 0.03),
xlab="Estimate Value", ylab="Probability Density",
main="Distribution of Estimates by Prior Knowledge Level")
# Add explanation text about density
mtext("Higher peaks = more concentrated estimates", side=3, line=0.5, cex=0.8)
# Add the density curves for each knowledge level
legend_text <- character(length(knowledge_levels))
for (i in 1:length(knowledge_levels)) {
pk <- knowledge_levels[i]
d <- density(all_estimates[[i]], adjust=1.2)  # Slightly smoother curves
lines(d, col=colors[i], lwd=2)
# Calculate mean and standard deviation for this knowledge level
mean_val <- mean(all_estimates[[i]])
sd_val <- sd(all_estimates[[i]])
legend_text[i] <- sprintf("PK = %.1f (Mean = %.1f, SD = %.1f)",
pk, mean_val, sd_val)
}
# Add a vertical line for the true value
abline(v=true_value, lty=2, col="black", lwd=2)
text(true_value, 0, "True Value", pos=3, offset=0.5)
# Add a legend with more information
legend("topright", legend=legend_text, col=colors, lwd=2, cex=0.9)
# Return the data for potential further analysis
invisible(all_estimates)
}
# Function to plot distribution of estimates by knowledge level
plot_estimate_distributions <- function(true_value = 100, n_samples = 5000,
knowledge_levels = c(0.1, 0.5, 0.9)) {
# Create an empty list to store the estimates for each knowledge level
all_estimates <- list()
# Generate estimates for each knowledge level
for (pk in knowledge_levels) {
# Generate n_samples estimates for this knowledge level
estimates <- sapply(rep(pk, n_samples), dIndividualFirstEstimate,
true_value = true_value)
# Store the estimates
all_estimates[[as.character(pk)]] <- estimates
}
# Set up the plotting area with multiple colors
colors <- c("red", "green", "blue")[1:length(knowledge_levels)]
# Find a more reasonable range for x-axis
# Calculate the 1st and 99th percentiles across all estimates to avoid extreme outliers
all_values <- unlist(all_estimates)
q_low <- quantile(all_values, 0.01)
q_high <- quantile(all_values, 0.99)
# Set range to include true value and reasonable range of estimates
x_min <- min(q_low, true_value * 0.5)
x_max <- max(q_high, true_value * 1.5)
# Create an empty plot
plot(1, type="n", xlim=c(x_min, x_max), ylim=c(0, 0.03),
xlab="Estimate Value", ylab="Probability Density",
main="Distribution of Estimates by Prior Knowledge Level")
# Add explanation text about density
mtext("Higher peaks = more concentrated estimates", side=3, line=0.5, cex=0.8)
# Add the density curves for each knowledge level
legend_text <- character(length(knowledge_levels))
for (i in 1:length(knowledge_levels)) {
pk <- knowledge_levels[i]
d <- density(all_estimates[[i]], adjust=1.2)  # Slightly smoother curves
lines(d, col=colors[i], lwd=2)
# Calculate mean and standard deviation for this knowledge level
mean_val <- mean(all_estimates[[i]])
sd_val <- sd(all_estimates[[i]])
legend_text[i] <- sprintf("PK = %.1f (Mean = %.1f, SD = %.1f)",
pk, mean_val, sd_val)
}
# Add a vertical line for the true value
abline(v=true_value, lty=2, col="black", lwd=2)
text(true_value, 0, "True Value", pos=3, offset=0.5)
# Add a legend with more information
legend("topright", legend=legend_text, col=colors, lwd=2, cex=0.9)
# Return the data for potential further analysis
invisible(all_estimates)
}
plot_estimate_distributions()
# Function to plot distribution of estimates by knowledge level
plot_estimate_distributions <- function(true_value = 100, n_samples = 10000,
knowledge_levels = c(0.1, 0.5, 0.9)) {
# Create an empty list to store the estimates for each knowledge level
all_estimates <- list()
# Generate estimates for each knowledge level
for (pk in knowledge_levels) {
# Generate n_samples estimates for this knowledge level
estimates <- sapply(rep(pk, n_samples), dIndividualFirstEstimate,
true_value = true_value)
# Store the estimates
all_estimates[[as.character(pk)]] <- estimates
}
# Set up the plotting area with multiple colors
colors <- c("red", "green", "blue")[1:length(knowledge_levels)]
# Find a more reasonable range for x-axis
# Calculate the 1st and 99th percentiles across all estimates to avoid extreme outliers
all_values <- unlist(all_estimates)
q_low <- quantile(all_values, 0.01)
q_high <- quantile(all_values, 0.99)
# Set range to include true value and reasonable range of estimates
x_min <- min(q_low, true_value * 0.5)
x_max <- max(q_high, true_value * 1.5)
# Create an empty plot
plot(1, type="n", xlim=c(x_min, x_max), ylim=c(0, 0.03),
xlab="Estimate Value", ylab="Probability Density",
main="Distribution of Estimates by Prior Knowledge Level")
# Add explanation text about density
mtext("Higher peaks = more concentrated estimates", side=3, line=0.5, cex=0.8)
# Add the density curves for each knowledge level
legend_text <- character(length(knowledge_levels))
for (i in 1:length(knowledge_levels)) {
pk <- knowledge_levels[i]
d <- density(all_estimates[[i]], adjust=1.2)  # Slightly smoother curves
lines(d, col=colors[i], lwd=2)
# Calculate mean and standard deviation for this knowledge level
mean_val <- mean(all_estimates[[i]])
sd_val <- sd(all_estimates[[i]])
legend_text[i] <- sprintf("PK = %.1f (Mean = %.1f, SD = %.1f)",
pk, mean_val, sd_val)
}
# Add a vertical line for the true value
abline(v=true_value, lty=2, col="black", lwd=2)
text(true_value, 0, "True Value", pos=3, offset=0.5)
# Add a legend with more information
legend("topright", legend=legend_text, col=colors, lwd=2, cex=0.9)
# Return the data for potential further analysis
invisible(all_estimates)
}
plot_estimate_distributions()
dunning_kruger_confidence <- function(PK) {
# Validate input
if (!is.numeric(PK) || any(PK < 0) || any(PK > 1)) {
stop("PK must be a numeric value between 0 and 1")
}
# Recalibrated coefficients to exactly hit both key points
a = 0.4      # Base confidence at zero knowledge
b = 1.0      # Controls initial rise
c = -1.2     # Creates the curve shape
d = 0.7      # Adjusts high-end behavior
# Calculate perceived percentile rank
perceived_rank <- a + b*PK + c*PK^2 + d*PK^3
# Ensure all values stay in [0,1] range
perceived_rank <- pmin(pmax(perceived_rank, 0), 1)
return(perceived_rank)
}
# Create a visualization
plot_dunning_kruger <- function() {
# Generate sequence of PK values for smooth curve
PK_values <- seq(0, 1, by=0.01)
confidence_values <- dunning_kruger_confidence(PK_values)
# Find where the curve crosses the identity line
crossings <- which(diff(sign(confidence_values - PK_values)) != 0)
crossing_point <- ifelse(length(crossings) > 0, PK_values[crossings[1] + 1], NA)
# Create plot
plot(PK_values, confidence_values, type="l", lwd=2, col="blue",
main="PK -> Confidence",
xlab="Prior Knowledge", ylab="Confidence",
xlim=c(0,1), ylim=c(0,1))
# Add identity line (where confidence = knowledge)
abline(0, 1, lty=2, col="gray")
# Add key points from the paper with correct PK values
points(0.324, 0.62, pch=19, col="red", cex=1.5)  # Bottom quartile (12th percentile)
points(0.692, 0.75, pch=19, col="green", cex=1.5)  # Top quartile (90th percentile)
# Mark crossing point
if(!is.na(crossing_point)) {
points(crossing_point, crossing_point, pch=19, col="purple", cex=1.5)
}
# Add legend
legend("bottomright",
legend=c("Dunning-Kruger Curve", "Accurate self-assessment (PK=CONF)",
"12th Percentile (PK=0.324)", "90th Percentile (PK=0.692)",
"Confidence = Knowledge Crossing"),
col=c("blue", "gray", "red", "green", "purple"),
lty=c(1,2,NA,NA,NA), pch=c(NA,NA,19,19,19),
cex=0.8)
# Add annotation about paper findings
text(0.324, 0.62, "  (0.324, 0.62)", pos=4, cex=0.8)
text(0.692, 0.75, "  (0.692, 0.75)", pos=4, cex=0.8)
# Add subtitle about distribution
title(sub="Calibrated for truncated normal distribution with mean=0.5, sd=0.15", cex.sub=0.8)
}
plot_dunning_kruger()
library("papaja")
r_refs("r-references.bib")
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
source("~/Uni/WiSe24/empra_2/script.R")
plot_dunning_kruger()
source(visualization.R)
source(script.R)
plot_dunning_kruger()
source(visualizations.R)
source(visualizations.R)
source(script.R)
source(visualizations.R)
source(script.R)
source(visualizations.R)
plot_dunning_kruger()
plot_dunning_kruger()
source("script.R")
source("visualizations.R")
plot_dunning_kruger()
source("script.R")
source("visualizations.R")
```{r }
source("script.R")
source("visualizations.R")
### Weight of Advice
This function determines how much individuals are influenced by advice based on their confidence and the distance between their first estimate and social information. It calculates a log-based distance measure, then adjusts the weight using confidence values and a tanh transformation. Higher confidence decreases the weight given to advice, while larger distances (with tanh transformation) can increase it. The result is constrained between 0 and 1.
# Determine the weight a person puts on advice
weightOfAdvice <- function(confidence, first_estimate, social_information) {
# Calculate log-based distance
log_ratio <- log(social_information / first_estimate)
distance <- abs(log_ratio)
# tanh of large distances approaches 1
weight_of_advice <- (1 - confidence) * (1 + confidence * tanh(distance))
weight_of_advice <- max(0, min(1, weight_of_advice))
return(weight_of_advice)
}
plot_weight_vs_confidence <- function(distances = c(0.1, 0.5, 1, 2, 5)) {
# Create a sequence of confidence values
conf_values <- seq(0, 1, by = 0.01)
# Set up the plot
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
xlab = "Confidence", ylab = "Weight of Advice",
main = "Weight of Advice vs. Confidence for Different Distances")
# Define a color palette for different distances
colors <- rainbow(length(distances))
# Add a line for each distance
for (i in 1:length(distances)) {
dist <- distances[i]
weights <- sapply(conf_values, function(conf) {
# For a fixed distance, calculate weight directly
weight <- (1 - conf) * (1 + conf * tanh(dist))
return(max(0, min(1, weight)))
})
lines(conf_values, weights, col = colors[i], lwd = 2)
}
# Add a legend
legend("topright", legend = paste("Distance =", distances),
col = colors, lwd = 2, cex = 0.8)
}
plot_weight_vs_confidence()
plot_weight_vs_confidence <- function(distances = c(0.1, 0.5, 1, 2, 5)) {
# Create a sequence of confidence values
conf_values <- seq(0, 1, by = 0.01)
# Set up the plot
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
xlab = "Confidence", ylab = "Weight of Advice",
main = "Weight of Advice vs. Confidence for Different Distances")
# Define a color palette for different distances
colors <- rainbow(length(distances))
# Add a line for each distance
for (i in 1:length(distances)) {
dist <- distances[i]
weights <- sapply(conf_values, function(conf) {
# For a fixed distance, calculate weight directly
weight <- (1 - conf) * (1 + conf * tanh(dist))
return(max(0, min(1, weight)))
})
lines(conf_values, weights, col = colors[i], lwd = 2)
}
# Add a legend
legend("topright", legend = paste("Distance =", distances),
col = colors, lwd = 2, cex = 0.8)
}
plot_weight_vs_confidence()
# Function to visualize Dunning-Kruger effect
plot_dunning_kruger <- function() {
# Generate sequence of PK values for smooth curve
PK_values <- seq(0, 1, by=0.01)
confidence_values <- dunning_kruger_confidence(PK_values)
# Find where the curve crosses the identity line
crossings <- which(diff(sign(confidence_values - PK_values)) != 0)
crossing_point <- ifelse(length(crossings) > 0, PK_values[crossings[1] + 1], NA)
# Create plot
plot(PK_values, confidence_values, type="l", lwd=2, col="blue",
main="Dunning-Kruger Effect",
xlab="Prior Knowledge", ylab="Confidence",
xlim=c(0,1), ylim=c(0,1))
# Add identity line (where confidence = knowledge)
abline(0, 1, lty=2, col="gray")
# Add key points from the paper with correct PK values
points(0.324, 0.62, pch=19, col="red", cex=1.5)  # Bottom quartile (12th percentile)
points(0.692, 0.75, pch=19, col="green", cex=1.5)  # Top quartile (90th percentile)
# Mark crossing point
if(!is.na(crossing_point)) {
points(crossing_point, crossing_point, pch=19, col="purple", cex=1.5)
}
# Add legend
legend("bottomright",
legend=c("Dunning-Kruger Curve", "Identity Line (accurate self-assessment)",
"12th Percentile (PK=0.324)", "90th Percentile (PK=0.692)",
"Confidence = Knowledge Crossing"),
col=c("blue", "gray", "red", "green", "purple"),
lty=c(1,2,NA,NA,NA), pch=c(NA,NA,19,19,19),
cex=0.8)
# Add annotation about paper findings
text(0.324, 0.62, "  (0.324, 0.62)", pos=4, cex=0.8)
text(0.692, 0.75, "  (0.692, 0.75)", pos=4, cex=0.8)
# Add subtitle about distribution
title(sub="Calibrated for truncated normal distribution with mean=0.5, sd=0.15", cex.sub=0.8)
}
plot_dunning_kruger()
source("~/Uni/WiSe24/empra_2/script.R")
ptruncnorm(x, a=0, b=1, mean=0.5, sd=0.15) * 100
# Convert PK to percentile, then back to PK scale
# This requires the truncnorm package
library(truncnorm)
source("script.R")
library(truncnorm)
# Function to visualize Dunning-Kruger effect
plot_dunning_kruger <- function() {
# Generate sequence of PK values for smooth curve
PK_values <- seq(0, 1, by=0.01)
confidence_values <- dunning_kruger_confidence(PK_values)
# Find where the curve crosses the identity line
crossings <- which(diff(sign(confidence_values - PK_values)) != 0)
crossing_point <- ifelse(length(crossings) > 0, PK_values[crossings[1] + 1], NA)
# Create plot
plot(PK_values, confidence_values, type="l", lwd=2, col="blue",
main="Dunning-Kruger Effect",
xlab="Prior Knowledge", ylab="Confidence",
xlim=c(0,1), ylim=c(0,1))
percentile_curve <- function(x) {
# Convert PK to percentile, then back to PK scale
# This requires the truncnorm package
ptruncnorm(x, a=0, b=1, mean=0.5, sd=0.15) * 100
}
curve(percentile_curve, 0, 1, add=TRUE, lty=2, col="gray")
# Add key points from the paper with correct PK values
points(0.324, 0.62, pch=19, col="red", cex=1.5)  # Bottom quartile (12th percentile)
points(0.692, 0.75, pch=19, col="green", cex=1.5)  # Top quartile (90th percentile)
# Mark crossing point
if(!is.na(crossing_point)) {
points(crossing_point, crossing_point, pch=19, col="purple", cex=1.5)
}
# Add legend
legend("bottomright",
legend=c("Dunning-Kruger Curve", "Identity Line (accurate self-assessment)",
"12th Percentile (PK=0.324)", "90th Percentile (PK=0.692)",
"Confidence = Knowledge Crossing"),
col=c("blue", "gray", "red", "green", "purple"),
lty=c(1,2,NA,NA,NA), pch=c(NA,NA,19,19,19),
cex=0.8)
# Add annotation about paper findings
text(0.324, 0.62, "  (0.324, 0.62)", pos=4, cex=0.8)
text(0.692, 0.75, "  (0.692, 0.75)", pos=4, cex=0.8)
# Add subtitle about distribution
title(sub="Calibrated for truncated normal distribution with mean=0.5, sd=0.15", cex.sub=0.8)
}
plot_dunning_kruger()
plot_dunning_kruger <- function() {
# Generate sequence of PK values for smooth curve
PK_values <- seq(0, 1, by=0.01)
confidence_values <- dunning_kruger_confidence(PK_values)
# Calculate the actual percentile rank for each PK value in our distribution
library(truncnorm)
percentile_ranks <- ptruncnorm(PK_values, a=0, b=1, mean=0.5, sd=0.15)
# Find where the DK curve crosses the perfect calibration curve
crossings <- which(diff(sign(confidence_values - percentile_ranks)) != 0)
if(length(crossings) > 0) {
crossing_point <- PK_values[crossings[1] + 1]
crossing_value <- percentile_ranks[crossings[1] + 1]
} else {
crossing_point <- NA
crossing_value <- NA
}
# Create plot
plot(PK_values, confidence_values, type="l", lwd=2, col="blue",
main="Dunning-Kruger Effect",
xlab="Prior Knowledge", ylab="Perceived Percentile Rank",
xlim=c(0,1), ylim=c(0,1))
# Add "perfect calibration" line - maps PK to actual percentile ranks
lines(PK_values, percentile_ranks, lty=2, col="gray")
# Add key points from the paper
points(0.324, 0.62, pch=19, col="red", cex=1.5)  # 12th percentile knowledge
points(0.692, 0.75, pch=19, col="green", cex=1.5)  # 90th percentile knowledge
# Mark crossing point if it exists
if(!is.na(crossing_point)) {
points(crossing_point, crossing_value, pch=19, col="purple", cex=1.5)
}
# Add legend
legend("bottomright",
legend=c("Dunning-Kruger Curve", "Perfect Calibration Line",
"12th Percentile (PK=0.324)", "90th Percentile (PK=0.692)",
"Crossing Point"),
col=c("blue", "gray", "red", "green", "purple"),
lty=c(1,2,NA,NA,NA), pch=c(NA,NA,19,19,19),
cex=0.7)
# Add annotation
text(0.324, 0.62, "  (0.324, 0.62)", pos=4, cex=0.8)
text(0.692, 0.75, "  (0.692, 0.75)", pos=4, cex=0.8)
# Add explanatory note
mtext("Calibrated for truncated normal distribution with mean=0.5, sd=0.15",
side=1, line=4, cex=0.8)
}
plot_dunning_kruger()
plot_dunning_kruger <- function() {
# Generate sequence of PK values for smooth curve
PK_values <- seq(0, 1, by=0.01)
confidence_values <- dunning_kruger_confidence(PK_values)
# Calculate the actual percentile rank for each PK value in our distribution
library(truncnorm)
percentile_ranks <- ptruncnorm(PK_values, a=0, b=1, mean=0.5, sd=0.15)
# Find where the DK curve crosses the perfect calibration curve
diff_values <- confidence_values - percentile_ranks
crossings <- which(diff(sign(diff_values)) != 0)
# Properly interpolate to find the exact crossing point
if(length(crossings) > 0) {
# Get the indices around the crossing
idx1 <- crossings[1]
idx2 <- idx1 + 1
# Linear interpolation to find exact crossing
t <- diff_values[idx1] / (diff_values[idx1] - diff_values[idx2])
crossing_pk <- PK_values[idx1] + t * (PK_values[idx2] - PK_values[idx1])
# At the crossing point, confidence equals percentile rank
crossing_confidence <- approx(PK_values, confidence_values, crossing_pk)$y
} else {
crossing_pk <- NA
crossing_confidence <- NA
}
# Create plot
plot(PK_values, confidence_values, type="l", lwd=2, col="blue",
main="Dunning-Kruger Effect",
xlab="Prior Knowledge", ylab="Perceived Percentile Rank",
xlim=c(0,1), ylim=c(0,1))
# Add "perfect calibration" line - maps PK to actual percentile ranks
lines(PK_values, percentile_ranks, lty=2, col="gray")
# Add key points from the paper
points(0.324, 0.62, pch=19, col="red", cex=1.5)  # 12th percentile knowledge
points(0.692, 0.75, pch=19, col="green", cex=1.5)  # 90th percentile knowledge
# Mark crossing point if it exists
if(!is.na(crossing_pk)) {
points(crossing_pk, crossing_confidence, pch=19, col="purple", cex=1.5)
# Label the crossing point
text(crossing_pk, crossing_confidence,
sprintf("  (%.2f, %.2f)", crossing_pk, crossing_confidence),
pos=4, cex=0.8)
}
# Add legend
legend("bottomright",
legend=c("Dunning-Kruger Curve", "Perfect Calibration Line",
"12th Percentile (PK=0.324)", "90th Percentile (PK=0.692)",
"Crossing Point"),
col=c("blue", "gray", "red", "green", "purple"),
lty=c(1,2,NA,NA,NA), pch=c(NA,NA,19,19,19),
cex=0.7)
# Add annotation
text(0.324, 0.62, "  (0.324, 0.62)", pos=4, cex=0.8)
text(0.692, 0.75, "  (0.692, 0.75)", pos=4, cex=0.8)
# Add explanatory note
mtext("Calibrated for truncated normal distribution with mean=0.5, sd=0.15",
side=1, line=4, cex=0.8)
}
plot_dunning_kruger()
